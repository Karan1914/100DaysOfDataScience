{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate the pearson relation between two variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grains' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d303dd183da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assign the 0th column of grains: width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Assign the 1st column of grains: length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grains' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform the necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assign the 0th column of grains: width\n",
    "width = grains[:,0]\n",
    "\n",
    "# Assign the 1st column of grains: length\n",
    "length = grains[:,1]\n",
    "\n",
    "# Scatter plot width vs length\n",
    "plt.scatter(width, length)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation\n",
    "correlation, pvalue = pearsonr(width,length)\n",
    "\n",
    "# Display the correlation\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic dimension of a dataset is the total number of features needed to approximate the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklear.decomposition import PCA\n",
    "model=PCA()\n",
    "model.fit(samples)\n",
    "features=range(pca.n_components)\n",
    "plt.bar(features,pca.explained_variance_)\n",
    "plt.xticks(features)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('PCA feature')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to put an arrow to the first component of PCA\n",
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Variance of the PCA features\n",
    "The fish dataset is 6-dimensional. But what is its intrinsic dimension? \n",
    "Make a plot of the variances of the PCA features to find out.\n",
    "As before, samples is a 2D array, where each row represents a fish. \n",
    "You'll need to standardize the features first.\"\"\"\n",
    "\n",
    "\"\"\"Create an instance of StandardScaler called scaler.\n",
    "Create a PCA instance called pca.\n",
    "Use the make_pipeline() function to create a pipeline chaining scaler and pca.\n",
    "Use the .fit() method of pipeline to fit it to the fish samples samples.\n",
    "Extract the number of components used using the .n_components_ attribute of pca. \n",
    "Place this inside a range() function and store the result as features.\n",
    "Use the plt.bar() function to plot the explained variances, \n",
    "with features on the x-axis and pca.explained_variance_ on the y-axis.\"\"\"\n",
    "\n",
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction of the fish measurements\n",
    "# select only 2 pca components\n",
    "\"\"\"\n",
    "Import PCA from sklearn.decomposition.\n",
    "Create a PCA instance called pca with n_components=2.\n",
    "Use the .fit() method of pca to fit it to the scaled fish measurements scaled_samples.\n",
    "Use the .transform() method of pca to transform the scaled_samples. Assign the result to pca_features.\"\"\"\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA model with 2 components: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance to the scaled samples\n",
    "pca.fit(scaled_samples)\n",
    "\n",
    "# Transform the scaled samples: pca_features\n",
    "pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "# Print the shape of pca_features\n",
    "print(pca_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A tf-idf word-frequency array\n",
    "In this exercise, you'll create a tf-idf word frequency array for a toy collection of documents. \n",
    "For this, use the TfidfVectorizer from sklearn. \n",
    "It transforms a list of documents into a word frequency array, \n",
    "which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects.\n",
    "\n",
    "You are given a list documents of toy documents about pets. \n",
    "Its contents have been printed in the IPython Shell.\"\"\"\n",
    "\n",
    "\"\"\"Import TfidfVectorizer from sklearn.feature_extraction.text.\n",
    "Create a TfidfVectorizer instance called tfidf.\n",
    "Apply .fit_transform() method of tfidf to documents and assign the result to csr_mat. \n",
    "This is a word-frequency array in csr_matrix format.\n",
    "Inspect csr_mat by calling its .toarray() method and printing the result. \n",
    "This has been done for you.\n",
    "The columns of the array correspond to words. \n",
    "Get the list of words by calling the .get_feature_names() method of tfidf, and assign the result to words.\"\"\"\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = TfidfVectorizer() \n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "# Print words\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
