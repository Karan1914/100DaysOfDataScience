{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwtEnYMDtxJJ3rdbI3v8GT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karan1914/100DaysOfDataScience/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FICTbHBbbLJh"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0zfKh3_bfI4"
      },
      "source": [
        "corpus = '    This is my duMmy Dataset. It is part of taught course on text mining.   '\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vC02k7z8bsm_",
        "outputId": "154397ed-a97a-4871-fdbb-a485e7759c6b"
      },
      "source": [
        "#1. to lowercase\n",
        "corpus = corpus.lower()\n",
        "\n",
        "corpus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'    this is my dummy dataset. it is part of taught course on text mining.   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UNfnhzw3bxwq",
        "outputId": "6820dab4-238a-414f-ac8d-da1b0df5f2b2"
      },
      "source": [
        "#2 remove white spaces\n",
        "corpus = corpus.strip()\n",
        "\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is my dummy dataset. it is part of taught course on text mining.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G5ENcIShbzrZ",
        "outputId": "0d008899-b678-422c-8dee-a140d623cd9e"
      },
      "source": [
        "#3 remove punctuations\n",
        "from string import punctuation as punc\n",
        "punc"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZcVavMLb0ka",
        "outputId": "a96db9bf-e217-4526-8806-fbdfb1242203"
      },
      "source": [
        "for ch in corpus:\n",
        "    if ch in punc:\n",
        "        corpus = corpus.replace(ch, '')\n",
        "        \n",
        "print(corpus)  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is my dummy dataset it is part of taught course on text mining\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJHh7-FBb_FP",
        "outputId": "f5eeb12e-7724-4c36-fd65-e6d925a8f212"
      },
      "source": [
        "#4 stop words removal\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "print(ENGLISH_STOP_WORDS)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozenset({'ltd', 'its', 'mill', 'those', 'because', 'describe', 'else', 'his', 'call', 'hers', 'etc', 'indeed', 'put', 'ourselves', 'here', 'anywhere', 'sometimes', 'bill', 'perhaps', 'six', 'noone', 'becoming', 'can', 'him', 'whereby', 'fire', 'onto', 'last', 'otherwise', 'whose', 'part', 'even', 'next', 'was', 'again', 'there', 'front', 'my', 'show', 'too', 'toward', 'made', 'alone', 'except', 'full', 'one', 'all', 'same', 'anything', 'thence', 'nor', 'that', 'such', 'under', 'empty', 'within', 'themselves', 'others', 'take', 'amoungst', 'co', 'before', 'done', 'it', 'than', 'thereby', 'therefore', 'anyway', 'but', 'about', 'nothing', 'nowhere', 'mostly', 'throughout', 'during', 'were', 'someone', 'something', 'so', 'the', 'am', 'cannot', 'over', 'yourself', 'well', 'still', 'wherever', 'beside', 'beyond', 'if', 'why', 'get', 'less', 'must', 'very', 'namely', 'when', 'their', 'thereupon', 'in', 'several', 'thru', 'will', 'and', 'of', 'once', 'eleven', 'third', 'should', 'side', 'please', 'two', 'your', 'go', 'amongst', 'moreover', 'system', 'a', 'whom', 'on', 'an', 'while', 'been', 'mine', 'serious', 'ours', 'thick', 'un', 'therein', 'do', 'then', 'us', 'nobody', 'towards', 'twelve', 'many', 'whether', 'up', 'whither', 'amount', 'couldnt', 'neither', 'for', 'see', 'between', 'sixty', 'ie', 'which', 'enough', 'itself', 'hundred', 'eg', 'keep', 'her', 'rather', 'ten', 'this', 'together', 'upon', 'per', 'behind', 're', 'further', 'whatever', 'our', 'own', 'another', 'interest', 'fifty', 'had', 'every', 'due', 'sometime', 'beforehand', 'whoever', 'both', 'first', 'without', 'eight', 'nevertheless', 'although', 'latterly', 'more', 'myself', 'not', 'seem', 'somewhere', 'whereafter', 'though', 'being', 'some', 'off', 'twenty', 'fill', 'find', 'is', 'give', 'hence', 'where', 'me', 'these', 'may', 'at', 'latter', 'whence', 'across', 'against', 'you', 'could', 'back', 'i', 'around', 'seeming', 'after', 'he', 'seems', 'would', 'hereafter', 'bottom', 'inc', 'thereafter', 'as', 'either', 'through', 'whenever', 'with', 'be', 'might', 'below', 'con', 'cant', 'also', 'any', 'fifteen', 'have', 'each', 'among', 'since', 'afterwards', 'seemed', 'to', 'always', 'name', 'anyhow', 'now', 'cry', 'yours', 'everyone', 'thus', 'few', 'four', 'besides', 'ever', 'everything', 'thin', 'nine', 'became', 'move', 'almost', 'has', 'herein', 'himself', 'what', 'yet', 'hereby', 'often', 'whole', 'de', 'become', 'three', 'somehow', 'until', 'sincere', 'along', 'top', 'only', 'much', 'already', 'how', 'wherein', 'she', 'herself', 'them', 'down', 'by', 'everywhere', 'hasnt', 'above', 'who', 'are', 'former', 'from', 'out', 'becomes', 'five', 'or', 'whereupon', 'no', 'forty', 'least', 'formerly', 'via', 'other', 'meanwhile', 'elsewhere', 'however', 'hereupon', 'into', 'most', 'none', 'we', 'yourselves', 'never', 'they', 'whereas', 'detail', 'found', 'anyone'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EE8h8NccUUz",
        "outputId": "3c7f1f6a-a3cc-4a12-9ef9-4fbd78b72303"
      },
      "source": [
        "words = corpus.split(' ')\n",
        "\n",
        "for word in words:\n",
        "    if word in ENGLISH_STOP_WORDS:\n",
        "        words.remove(word)\n",
        "        \n",
        "print(words)  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'dummy', 'dataset', 'is', 'of', 'taught', 'course', 'text', 'mining']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW201fVpcccr",
        "outputId": "36c20590-4f09-4328-8202-acea51f9ea5d"
      },
      "source": [
        "corpus = '<html><head></head><body><h1>Paragraph Heading</h1><p>This is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>'\n",
        "print('Raw data: ', corpus)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw data:  <html><head></head><body><h1>Paragraph Heading</h1><p>This is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwLmsrjuemhZ",
        "outputId": "6d6b2048-c29b-4015-cb52-71969fb341d4"
      },
      "source": [
        "import re\n",
        "tags = re.compile(r'<.*?>')\n",
        "corpus = tags.sub('', corpus)\n",
        "\n",
        "prices = re.compile(r'(USD|\\$)[0-9]+')\n",
        "corpus = prices.sub('', corpus)\n",
        "\n",
        "print('normalized data: ', corpus)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalized data:  Paragraph HeadingThis is some text. The original price was  but now only   This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQGOJhlDfQxM",
        "outputId": "1a4fcfa1-8759-4d45-b67e-5f1ac1cf58a9"
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "pstemmer = PorterStemmer()\n",
        "for i in range(0, len(words)):\n",
        "    words[i] = pstemmer.stem(words[i])\n",
        "print(words)\n",
        "\n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'dummi', 'dataset', 'is', 'of', 'taught', 'cours', 'text', 'mine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZkFoRrffdL"
      },
      "source": [
        "import nltk\n",
        "#nltk.download('all')\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB7pvLwyfSmU",
        "outputId": "0ae68387-d0fa-453e-a844-ae7716ee0432"
      },
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "for i in range(0, len(words[i])):\n",
        "    words[i] = lemma.lemmatize(words[i], pos='v')\n",
        "print(words) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['be', 'dummi', 'dataset', 'is', 'of', 'taught', 'cours', 'text', 'mine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2R6XrlSfT2u",
        "outputId": "74cb2a7b-0ab2-4815-8750-c9379dfb57ae"
      },
      "source": [
        "\n",
        "corpus = 'i like this table in my room'\n",
        "words = corpus.split(' ')\n",
        "\n",
        "from nltk import pos_tag\n",
        "tags = pos_tag(words)\n",
        "\n",
        "for i in range(0, len(tags)):\n",
        "    if tags[i][1] == 'DT':\n",
        "        print(tags[i])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('this', 'DT')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X15YKMhkgMqJ",
        "outputId": "d01f6570-ccc9-446b-be14-7ada06ef481a"
      },
      "source": [
        "corpus = 'He owes me 22.50 dollars. Which is due by the next day.'\n",
        "print(corpus.split('.'))\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "print(sent_tokenize(corpus))\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He owes me 22', '50 dollars', ' Which is due by the next day', '']\n",
            "['He owes me 22.50 dollars.', 'Which is due by the next day.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJuWmuUwgrKs",
        "outputId": "2f01bfd9-14e4-45cf-b2f5-4fdcfdd155b2"
      },
      "source": [
        "for sent in sent_tokenize(corpus):\n",
        "    print(word_tokenize(sent))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'owes', 'me', '22.50', 'dollars', '.']\n",
            "['Which', 'is', 'due', 'by', 'the', 'next', 'day', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNHdVMAYgypk",
        "outputId": "e98ff351-9215-4dff-cc10-61f5b08f605d"
      },
      "source": [
        "\n",
        "import requests\n",
        "session = requests.Session()\n",
        "session.trust_env = False\n",
        "\n",
        "#proxies = {\n",
        "#          'http' : 'http://192.168.12.84:808',\n",
        "#          'https' : 'https://192.168.23.38:808'\n",
        "#       }\n",
        "\n",
        "response = session.get('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "print(response)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoqmRZzphCw0"
      },
      "source": [
        "#print(response.text)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFE7jNPvibZm"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvkgEmGak_63"
      },
      "source": [
        "### Types of Text Representation\n",
        "  - Frequency based Bag of words\n",
        "  - Binary Based Bag of Words\n",
        "  - Log Frequency Distribution\n",
        "  - TF- IDF Frequency Distribution\n",
        "\n",
        "    - TF - IDF = TF * IDF\n",
        "      - TF = Count (w in Doc) // Doc Size\n",
        "      - IDF = log | M + 1 // k |\n",
        "        - M : Total Number of Documents in a Dataset\n",
        "        - k : Documents that have this particular word\n",
        "\n",
        "        - IDF is important to reduce the impact of frequently occuring words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wV9lYxhmONE",
        "outputId": "7f4c5456-7f92-4b4b-da40-44655e452df4"
      },
      "source": [
        "# Frequency Based Distribution for One Document \n",
        "\n",
        "corpus = ['text text mining is interesting']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "\n",
        "X = vec.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 2]]\n",
            "['interesting', 'is', 'mining', 'text']\n",
            "4\n",
            "{'text': 3, 'mining': 2, 'is': 1, 'interesting': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfrD_VH0noFO",
        "outputId": "5ba3029c-fbe6-4aee-a8e6-e025cc02191c"
      },
      "source": [
        "\n",
        "# Frequency Based Distribution for Multi Document \n",
        "\n",
        "corpus = ['text text mining is interesting',\n",
        "          'text mining is the same as data mining',\n",
        "          'text and data mining have few differences']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "\n",
        "X = vec.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 1 1 1 0 2 0]\n",
            " [0 1 1 0 0 0 0 1 2 1 1 1]\n",
            " [1 0 1 1 1 1 0 0 1 0 1 0]]\n",
            "['and', 'as', 'data', 'differences', 'few', 'have', 'interesting', 'is', 'mining', 'same', 'text', 'the']\n",
            "12\n",
            "{'text': 10, 'mining': 8, 'is': 7, 'interesting': 6, 'the': 11, 'same': 9, 'as': 1, 'data': 2, 'and': 0, 'have': 5, 'few': 4, 'differences': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J5pIeRpn0Bq",
        "outputId": "ca7ea35e-8a0f-4eed-ac4e-a7602f85f981"
      },
      "source": [
        "# Updating Parameters\n",
        "# Binary Frequency Based Distribution for Multi Document \n",
        "\n",
        "corpus = ['text text mining is interesting',\n",
        "          'text mining is the same as data mining',\n",
        "          'text and data mining have few differences']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer(binary = 'true', max_df = 2, min_df = 2, max_features =2, ngram_range=(1,3))\n",
        "\n",
        "X = vec.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)\n",
        "\n",
        "# Parameters name : 1) Binary = True\n",
        "# 2) max_df = 2  :  maximum document frequency\n",
        "# 3) min_df = 2   : minimum document frequency\n",
        "# max_features = 2 :  Best n features in all documents\n",
        "# ngram_range = (1,3) : Unigrams, Bigrams, Trigrams"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [1 1]\n",
            " [1 1]]\n",
            "['data', 'data mining']\n",
            "2\n",
            "{'data': 0, 'data mining': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkALm-bUoS1Y",
        "outputId": "2c9465f3-b4cf-45d9-f89a-64313a1af48c"
      },
      "source": [
        "# TF-IDF Frequency Based Distribution for Multi Document \n",
        "\n",
        "corpus = ['text text mining is interesting',\n",
        "          'text mining is the same as data mining',\n",
        "          'text and data mining have few differences']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer()\n",
        "X = vec.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.54861178 0.4172334  0.32401895 0.         0.64803791 0.        ]\n",
            " [0.         0.41166084 0.31307868 0.         0.         0.\n",
            "  0.         0.31307868 0.48626704 0.41166084 0.24313352 0.41166084]\n",
            " [0.43535684 0.         0.3311001  0.43535684 0.43535684 0.43535684\n",
            "  0.         0.         0.25712876 0.         0.25712876 0.        ]]\n",
            "['and', 'as', 'data', 'differences', 'few', 'have', 'interesting', 'is', 'mining', 'same', 'text', 'the']\n",
            "12\n",
            "{'text': 10, 'mining': 8, 'is': 7, 'interesting': 6, 'the': 11, 'same': 9, 'as': 1, 'data': 2, 'and': 0, 'have': 5, 'few': 4, 'differences': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqsd-0Lp6hU"
      },
      "source": [
        "\n",
        "\n",
        "# Dummy dataset Import and performing operations\n",
        "\n",
        "corpus = open('E:\\\\dataset.txt').read()\n",
        "docs = corpus.split('\\n')\n",
        "\n",
        "input_X, y = [], []\n",
        "\n",
        "for doc in docs:\n",
        "    i, l = doc.split(':')\n",
        "    input_X.append(i.strip())\n",
        "    y.append(l.strip())\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer(binary = 'true', max_df = 2, min_df = 2, max_features =2, ngram_range=(1,3))\n",
        "X = vec.fit_transform(input_X)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cYHJ6Fru7F"
      },
      "source": [
        "### Lazy or Instance base machine Learning Models : KNN : Uses data to find the pattern \n",
        "### Model Based : Doesnot require data to find the pattern\n",
        "\n",
        "  - Non Parametric :  Takes more time , but comes up with a solution : Do not look into the data with a predefined solution in a mind, hence comes up with a dynamic solution\n",
        "  - Parametric are fast "
      ]
    }
  ]
}