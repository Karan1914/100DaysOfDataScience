{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUGDEqH/Rb7bL/bBl7xbVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karan1914/100DaysOfDataScience/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FICTbHBbbLJh"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0zfKh3_bfI4"
      },
      "source": [
        "corpus = '    This is my duMmy Dataset. It is part of taught course on text mining.   '\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vC02k7z8bsm_",
        "outputId": "154397ed-a97a-4871-fdbb-a485e7759c6b"
      },
      "source": [
        "#1. to lowercase\n",
        "corpus = corpus.lower()\n",
        "\n",
        "corpus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'    this is my dummy dataset. it is part of taught course on text mining.   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UNfnhzw3bxwq",
        "outputId": "6820dab4-238a-414f-ac8d-da1b0df5f2b2"
      },
      "source": [
        "#2 remove white spaces\n",
        "corpus = corpus.strip()\n",
        "\n",
        "corpus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is my dummy dataset. it is part of taught course on text mining.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G5ENcIShbzrZ",
        "outputId": "0d008899-b678-422c-8dee-a140d623cd9e"
      },
      "source": [
        "#3 remove punctuations\n",
        "from string import punctuation as punc\n",
        "punc"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZcVavMLb0ka",
        "outputId": "a96db9bf-e217-4526-8806-fbdfb1242203"
      },
      "source": [
        "for ch in corpus:\n",
        "    if ch in punc:\n",
        "        corpus = corpus.replace(ch, '')\n",
        "        \n",
        "print(corpus)  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this is my dummy dataset it is part of taught course on text mining\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJHh7-FBb_FP",
        "outputId": "f5eeb12e-7724-4c36-fd65-e6d925a8f212"
      },
      "source": [
        "#4 stop words removal\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "print(ENGLISH_STOP_WORDS)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozenset({'ltd', 'its', 'mill', 'those', 'because', 'describe', 'else', 'his', 'call', 'hers', 'etc', 'indeed', 'put', 'ourselves', 'here', 'anywhere', 'sometimes', 'bill', 'perhaps', 'six', 'noone', 'becoming', 'can', 'him', 'whereby', 'fire', 'onto', 'last', 'otherwise', 'whose', 'part', 'even', 'next', 'was', 'again', 'there', 'front', 'my', 'show', 'too', 'toward', 'made', 'alone', 'except', 'full', 'one', 'all', 'same', 'anything', 'thence', 'nor', 'that', 'such', 'under', 'empty', 'within', 'themselves', 'others', 'take', 'amoungst', 'co', 'before', 'done', 'it', 'than', 'thereby', 'therefore', 'anyway', 'but', 'about', 'nothing', 'nowhere', 'mostly', 'throughout', 'during', 'were', 'someone', 'something', 'so', 'the', 'am', 'cannot', 'over', 'yourself', 'well', 'still', 'wherever', 'beside', 'beyond', 'if', 'why', 'get', 'less', 'must', 'very', 'namely', 'when', 'their', 'thereupon', 'in', 'several', 'thru', 'will', 'and', 'of', 'once', 'eleven', 'third', 'should', 'side', 'please', 'two', 'your', 'go', 'amongst', 'moreover', 'system', 'a', 'whom', 'on', 'an', 'while', 'been', 'mine', 'serious', 'ours', 'thick', 'un', 'therein', 'do', 'then', 'us', 'nobody', 'towards', 'twelve', 'many', 'whether', 'up', 'whither', 'amount', 'couldnt', 'neither', 'for', 'see', 'between', 'sixty', 'ie', 'which', 'enough', 'itself', 'hundred', 'eg', 'keep', 'her', 'rather', 'ten', 'this', 'together', 'upon', 'per', 'behind', 're', 'further', 'whatever', 'our', 'own', 'another', 'interest', 'fifty', 'had', 'every', 'due', 'sometime', 'beforehand', 'whoever', 'both', 'first', 'without', 'eight', 'nevertheless', 'although', 'latterly', 'more', 'myself', 'not', 'seem', 'somewhere', 'whereafter', 'though', 'being', 'some', 'off', 'twenty', 'fill', 'find', 'is', 'give', 'hence', 'where', 'me', 'these', 'may', 'at', 'latter', 'whence', 'across', 'against', 'you', 'could', 'back', 'i', 'around', 'seeming', 'after', 'he', 'seems', 'would', 'hereafter', 'bottom', 'inc', 'thereafter', 'as', 'either', 'through', 'whenever', 'with', 'be', 'might', 'below', 'con', 'cant', 'also', 'any', 'fifteen', 'have', 'each', 'among', 'since', 'afterwards', 'seemed', 'to', 'always', 'name', 'anyhow', 'now', 'cry', 'yours', 'everyone', 'thus', 'few', 'four', 'besides', 'ever', 'everything', 'thin', 'nine', 'became', 'move', 'almost', 'has', 'herein', 'himself', 'what', 'yet', 'hereby', 'often', 'whole', 'de', 'become', 'three', 'somehow', 'until', 'sincere', 'along', 'top', 'only', 'much', 'already', 'how', 'wherein', 'she', 'herself', 'them', 'down', 'by', 'everywhere', 'hasnt', 'above', 'who', 'are', 'former', 'from', 'out', 'becomes', 'five', 'or', 'whereupon', 'no', 'forty', 'least', 'formerly', 'via', 'other', 'meanwhile', 'elsewhere', 'however', 'hereupon', 'into', 'most', 'none', 'we', 'yourselves', 'never', 'they', 'whereas', 'detail', 'found', 'anyone'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EE8h8NccUUz",
        "outputId": "3c7f1f6a-a3cc-4a12-9ef9-4fbd78b72303"
      },
      "source": [
        "words = corpus.split(' ')\n",
        "\n",
        "for word in words:\n",
        "    if word in ENGLISH_STOP_WORDS:\n",
        "        words.remove(word)\n",
        "        \n",
        "print(words)  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'dummy', 'dataset', 'is', 'of', 'taught', 'course', 'text', 'mining']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW201fVpcccr",
        "outputId": "36c20590-4f09-4328-8202-acea51f9ea5d"
      },
      "source": [
        "corpus = '<html><head></head><body><h1>Paragraph Heading</h1><p>This is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>'\n",
        "print('Raw data: ', corpus)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw data:  <html><head></head><body><h1>Paragraph Heading</h1><p>This is some text. <a href=\"\">The original price was $500 but now only USD250 </a> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is <em>some text.</em> <strong>This is some text.</strong> This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. </p></body></html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwLmsrjuemhZ",
        "outputId": "6d6b2048-c29b-4015-cb52-71969fb341d4"
      },
      "source": [
        "import re\n",
        "tags = re.compile(r'<.*?>')\n",
        "corpus = tags.sub('', corpus)\n",
        "\n",
        "prices = re.compile(r'(USD|\\$)[0-9]+')\n",
        "corpus = prices.sub('', corpus)\n",
        "\n",
        "print('normalized data: ', corpus)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalized data:  Paragraph HeadingThis is some text. The original price was  but now only   This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. This is some text. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQGOJhlDfQxM",
        "outputId": "1a4fcfa1-8759-4d45-b67e-5f1ac1cf58a9"
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "pstemmer = PorterStemmer()\n",
        "for i in range(0, len(words)):\n",
        "    words[i] = pstemmer.stem(words[i])\n",
        "print(words)\n",
        "\n",
        " "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'dummi', 'dataset', 'is', 'of', 'taught', 'cours', 'text', 'mine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZkFoRrffdL"
      },
      "source": [
        "import nltk\n",
        "#nltk.download('all')\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB7pvLwyfSmU",
        "outputId": "0ae68387-d0fa-453e-a844-ae7716ee0432"
      },
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "for i in range(0, len(words[i])):\n",
        "    words[i] = lemma.lemmatize(words[i], pos='v')\n",
        "print(words) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['be', 'dummi', 'dataset', 'is', 'of', 'taught', 'cours', 'text', 'mine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2R6XrlSfT2u",
        "outputId": "74cb2a7b-0ab2-4815-8750-c9379dfb57ae"
      },
      "source": [
        "\n",
        "corpus = 'i like this table in my room'\n",
        "words = corpus.split(' ')\n",
        "\n",
        "from nltk import pos_tag\n",
        "tags = pos_tag(words)\n",
        "\n",
        "for i in range(0, len(tags)):\n",
        "    if tags[i][1] == 'DT':\n",
        "        print(tags[i])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('this', 'DT')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X15YKMhkgMqJ",
        "outputId": "d01f6570-ccc9-446b-be14-7ada06ef481a"
      },
      "source": [
        "corpus = 'He owes me 22.50 dollars. Which is due by the next day.'\n",
        "print(corpus.split('.'))\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "print(sent_tokenize(corpus))\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He owes me 22', '50 dollars', ' Which is due by the next day', '']\n",
            "['He owes me 22.50 dollars.', 'Which is due by the next day.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJuWmuUwgrKs",
        "outputId": "2f01bfd9-14e4-45cf-b2f5-4fdcfdd155b2"
      },
      "source": [
        "for sent in sent_tokenize(corpus):\n",
        "    print(word_tokenize(sent))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'owes', 'me', '22.50', 'dollars', '.']\n",
            "['Which', 'is', 'due', 'by', 'the', 'next', 'day', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNHdVMAYgypk",
        "outputId": "e98ff351-9215-4dff-cc10-61f5b08f605d"
      },
      "source": [
        "\n",
        "import requests\n",
        "session = requests.Session()\n",
        "session.trust_env = False\n",
        "\n",
        "#proxies = {\n",
        "#          'http' : 'http://192.168.12.84:808',\n",
        "#          'https' : 'https://192.168.23.38:808'\n",
        "#       }\n",
        "\n",
        "response = session.get('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "print(response)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoqmRZzphCw0"
      },
      "source": [
        "#print(response.text)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFE7jNPvibZm"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvkgEmGak_63"
      },
      "source": [
        "### Types of Text Representation\n",
        "  - Frequency based Bag of words\n",
        "  - Binary Based Bag of Words\n",
        "  - Log Frequency Distribution\n",
        "  - TF- IDF Frequency Distribution\n",
        "\n",
        "    - TF - IDF = TF * IDF\n",
        "      - TF = Count (w in Doc) // Doc Size\n",
        "      - IDF = log | M + 1 // k |\n",
        "        - M : Total Number of Documents in a Dataset\n",
        "        - k : Documents that have this particular word\n",
        "\n",
        "        - IDF is important to reduce the impact of frequently occuring words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wV9lYxhmONE",
        "outputId": "7f4c5456-7f92-4b4b-da40-44655e452df4"
      },
      "source": [
        "# Frequency Based Distribution for One Document \n",
        "\n",
        "corpus = ['text text mining is interesting']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "\n",
        "X = vec.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 2]]\n",
            "['interesting', 'is', 'mining', 'text']\n",
            "4\n",
            "{'text': 3, 'mining': 2, 'is': 1, 'interesting': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfrD_VH0noFO",
        "outputId": "5ba3029c-fbe6-4aee-a8e6-e025cc02191c"
      },
      "source": [
        "\n",
        "# Frequency Based Distribution for Multi Document \n",
        "\n",
        "corpus = ['text text mining is interesting',\n",
        "          'text mining is the same as data mining',\n",
        "          'text and data mining have few differences']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "\n",
        "X = vec.fit_transform(corpus)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 1 1 1 0 2 0]\n",
            " [0 1 1 0 0 0 0 1 2 1 1 1]\n",
            " [1 0 1 1 1 1 0 0 1 0 1 0]]\n",
            "['and', 'as', 'data', 'differences', 'few', 'have', 'interesting', 'is', 'mining', 'same', 'text', 'the']\n",
            "12\n",
            "{'text': 10, 'mining': 8, 'is': 7, 'interesting': 6, 'the': 11, 'same': 9, 'as': 1, 'data': 2, 'and': 0, 'have': 5, 'few': 4, 'differences': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J5pIeRpn0Bq",
        "outputId": "ca7ea35e-8a0f-4eed-ac4e-a7602f85f981"
      },
      "source": [
        "# Updating Parameters\n",
        "# Binary Frequency Based Distribution for Multi Document \n",
        "\n",
        "corpus = ['text text mining is interesting',\n",
        "          'text mining is the same as data mining',\n",
        "          'text and data mining have few differences']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer(binary = 'true', max_df = 2, min_df = 2, max_features =2, ngram_range=(1,3))\n",
        "\n",
        "X = vec.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)\n",
        "\n",
        "# Parameters name : 1) Binary = True\n",
        "# 2) max_df = 2  :  maximum document frequency\n",
        "# 3) min_df = 2   : minimum document frequency\n",
        "# max_features = 2 :  Best n features in all documents\n",
        "# ngram_range = (1,3) : Unigrams, Bigrams, Trigrams"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [1 1]\n",
            " [1 1]]\n",
            "['data', 'data mining']\n",
            "2\n",
            "{'data': 0, 'data mining': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkALm-bUoS1Y",
        "outputId": "2c9465f3-b4cf-45d9-f89a-64313a1af48c"
      },
      "source": [
        "# TF-IDF Frequency Based Distribution for Multi Document \n",
        "\n",
        "corpus = ['text text mining is interesting',\n",
        "          'text mining is the same as data mining',\n",
        "          'text and data mining have few differences']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer()\n",
        "X = vec.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.54861178 0.4172334  0.32401895 0.         0.64803791 0.        ]\n",
            " [0.         0.41166084 0.31307868 0.         0.         0.\n",
            "  0.         0.31307868 0.48626704 0.41166084 0.24313352 0.41166084]\n",
            " [0.43535684 0.         0.3311001  0.43535684 0.43535684 0.43535684\n",
            "  0.         0.         0.25712876 0.         0.25712876 0.        ]]\n",
            "['and', 'as', 'data', 'differences', 'few', 'have', 'interesting', 'is', 'mining', 'same', 'text', 'the']\n",
            "12\n",
            "{'text': 10, 'mining': 8, 'is': 7, 'interesting': 6, 'the': 11, 'same': 9, 'as': 1, 'data': 2, 'and': 0, 'have': 5, 'few': 4, 'differences': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hqsd-0Lp6hU"
      },
      "source": [
        "\n",
        "\n",
        "# Dummy dataset Import and performing operations\n",
        "\n",
        "corpus = open('E:\\\\dataset.txt').read()\n",
        "docs = corpus.split('\\n')\n",
        "\n",
        "input_X, y = [], []\n",
        "\n",
        "for doc in docs:\n",
        "    i, l = doc.split(':')\n",
        "    input_X.append(i.strip())\n",
        "    y.append(l.strip())\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer(binary = 'true', max_df = 2, min_df = 2, max_features =2, ngram_range=(1,3))\n",
        "X = vec.fit_transform(input_X)\n",
        "\n",
        "print(X.toarray())\n",
        "print(vec.get_feature_names())\n",
        "print(len(vec.get_feature_names()))\n",
        "print(vec.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cYHJ6Fru7F"
      },
      "source": [
        "### Lazy or Instance base machine Learning Models : KNN : Uses data to find the pattern \n",
        "### Model Based : Doesnot require data to find the pattern\n",
        "\n",
        "  - Non Parametric :  Takes more time , but comes up with a solution : Do not look into the data with a predefined solution in a mind, hence comes up with a dynamic solution\n",
        "  - Parametric are fast "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayOQ1Pg9q4c5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeoYRnbYq4om"
      },
      "source": [
        "\n",
        "\n",
        "#reading data from the file\n",
        "corpus = open('E:\\\\dataset.txt').read()\n",
        "\n",
        "#separating data into documents and labels.\n",
        "docs = corpus.split('\\n')\n",
        "X, y = [], []\n",
        "for doc in docs:\n",
        "    i, l = doc.split(':')\n",
        "    X.append(i.strip())\n",
        "    y.append(l.strip())\n",
        "\n",
        "#Structure input data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "matrix_X = vec.fit_transform(X)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_R7FC8wq-yn"
      },
      "source": [
        "#Applying K-Nearest Neighbors classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(matrix_X[:5], y[:5])\n",
        "print('KNN Classifier, Label: ' + str(knn.predict(matrix_X[5])))\n",
        "print('KNN Classifier, prob.' + str(knn.predict_proba(matrix_X[5])))\n",
        "\n",
        "#Applying Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nbc = MultinomialNB()\n",
        "nbc.fit(matrix_X[:5], y[:5])\n",
        "print('Naive Bayes Classifier, Label: ' + str(nbc.predict(matrix_X[5])))\n",
        "print('Naive Bayes Classifier, prob.' + str(nbc.predict_proba(matrix_X[5])))\n",
        "\n",
        "#Applying Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(matrix_X[:5], y[:5])\n",
        "print('Decision Tree Classifier, Label: ' + str(dtc.predict(matrix_X[5])))\n",
        "print('Decision Tree Classifier, prob.' + str(dtc.predict_proba(matrix_X[5])))\n",
        "\n",
        "#Applying Linear Classifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "lc = SGDClassifier()\n",
        "lc.fit(matrix_X[:5], y[:5])\n",
        "print('Linear Classifier, Label: ' + str(lc.predict(matrix_X[5])))\n",
        "#Linear Classifier doesn't have the probability value as it operates differently\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZmrHP-PsKEH"
      },
      "source": [
        "# Tuning With parameters\n",
        "\n",
        "\n",
        "#Applying K-Nearest Neighbors classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 3, algorithm = 'kd_tree', weights = 'distance')\n",
        "knn.fit(matrix_X[:5], y[:5])\n",
        "print('KNN Classifier, Label: ' + str(knn.predict(matrix_X[5])))\n",
        "print('KNN Classifier, prob.' + str(knn.predict_proba(matrix_X[5])))\n",
        "\n",
        "#Applying Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nbc = MultinomialNB(alpha = 0.2, fit_prior = False, class_prior = [0.6, 0.4])\n",
        "nbc.fit(matrix_X[:5], y[:5])\n",
        "print('Naive Bayes Classifier, Label: ' + str(nbc.predict(matrix_X[5])))\n",
        "print('Naive Bayes Classifier, prob.' + str(nbc.predict_proba(matrix_X[5])))\n",
        "\n",
        "#Applying Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(max_depth = 2)\n",
        "dtc.fit(matrix_X[:5], y[:5])\n",
        "print('Decision Tree Classifier, Label: ' + str(dtc.predict(matrix_X[5])))\n",
        "print('Decision Tree Classifier, prob.' + str(dtc.predict_proba(matrix_X[5])))\n",
        "\n",
        "#Applying Linear Classifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "lc = SGDClassifier(max_iter = 1000)\n",
        "lc.fit(matrix_X[:5], y[:5])\n",
        "print('Linear Classifier, Label: ' + str(lc.predict(matrix_X[5])))\n",
        "#Linear Classifier doesn't have the probability value as it operates differently"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FktRCh7csN7D"
      },
      "source": [
        "# DATA TAken from UCI repository : Badges data : 294 instances\n",
        "# https://archive.ics.uci.edu/ml/datasets/Badges\n",
        "\n",
        "# Predicting Badges (+, - )\n",
        "\n",
        "corpus = open('E:\\\\badges.data').read()\n",
        "docs = corpus.split('\\n')\n",
        "X, y = [], [] \n",
        "for doc in docs:\n",
        "    l = doc[:1]\n",
        "    i = doc[2:]\n",
        "    X.append(i)\n",
        "    y.append(l)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "matrix_X = vec.fit_transform(X)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(matrix_X[:290], y[:290])\n",
        "#predicted labels of the last four documents\n",
        "print(knn.predict(matrix_X[290:])) \n",
        "#prediction probability of the two labels for each of the last four documents\n",
        "print(knn.predict_proba(matrix_X[290:])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaxlACadvP2o"
      },
      "source": [
        "# Predicting Sentiments\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#preparing data\n",
        "raw_data = pd.read_csv(\"D:\\\\Datasets\\\\imdb-dataset-of-50k-movie-reviews\\\\IMDB Dataset.csv\")\n",
        "\n",
        "review_docs = raw_data.iloc[:, 0]\n",
        "y = raw_data.iloc[:, 1]\n",
        "\n",
        "print(set(y))\n",
        "\n",
        "#structuring data\n",
        "tfidf = TfidfVectorizer(min_df = 20, max_df = 30000).fit_transform(review_docs)\n",
        "#print(tfidf)\n",
        "\n",
        "# Splitting data to training and testing\n",
        "training_docs, training_y, testing_docs, testing_y = train_test_split(tfidf, y, train_size = 0.8, shuffle = True)\n",
        "\n",
        "#training model\n",
        "clf = MultinomialNB().fit(training_docs, training_y)\n",
        "\n",
        "#predicting labels for last 10 instances\n",
        "pred_y = clf.predict(testing_docs)\n",
        "print('Predicted labels: ', clf.predict(testing_docs))\n",
        "\n",
        "#Actual labels of the last 10 instances\n",
        "print('Actual labels: ', testing_y)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc = accuracy_score(testing_y, pred_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gavbNT_6zksW"
      },
      "source": [
        "# K- Fold\n",
        "\n",
        "corpus = open('E:\\\\dataset.txt').read()\n",
        "docs = corpus.split('\\n')\n",
        "X, y = [], []\n",
        "for doc in docs:\n",
        "    i, l = doc.split(':')\n",
        "    X.append(i)\n",
        "    y.append(l)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "matrix_X = vec.fit_transform(X)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 1)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits = 3)\n",
        "for id_train, id_test in kf.split(matrix_X):\n",
        "    print('Train', id_train, 'Test', id_test)\n",
        "    train_X, test_X = matrix_X[id_train], matrix_X[id_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKyOZHmo1WBJ"
      },
      "source": [
        "\n",
        "# K-Fold on IMDB data\n",
        "\n",
        "\n",
        "acc = []\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits = 3)\n",
        "\n",
        "for train_ids, test_ids in kf.split(tfidf,y):\n",
        "    \n",
        "    train_docs, test_docs = tfidf[train_ids], tfidf[test_ids]\n",
        "    train_y, test_y = y[train_ids], y[test_ids]\n",
        "    clf = MultinomialNB().fit(train_docs, train_y)\n",
        "    pred_y = clf.predict(test_docs)\n",
        "    acc.append(accuracy_score(test_y, pred_y))\n",
        "\n",
        "total_accuracy = sum(acc)/5\n",
        "print(total_accuracy)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6J0RHPK2qZE"
      },
      "source": [
        "# Leave one out \n",
        "\n",
        "acc = []\n",
        "\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "loo = LeaveOneOut(n_splits = 3)\n",
        "\n",
        "for train_ids, test_ids in loo.split(tfidf):\n",
        "    \n",
        "    train_docs, test_docs = tfidf[train_ids], tfidf[test_ids]\n",
        "    train_y, test_y = y[train_ids], y[test_ids]\n",
        "    clf = MultinomialNB().fit(train_docs, train_y)\n",
        "    pred_y = clf.predict(test_docs)\n",
        "    acc.append(accuracy_score(test_y, pred_y))\n",
        "\n",
        "total_accuracy = sum(acc)/5\n",
        "print(total_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnFbz8Z52qg0"
      },
      "source": [
        "# Putting it altogether\n",
        "\n",
        "corpus = open('E:\\\\badges.data').read()\n",
        "docs = corpus.split('\\n')\n",
        "X, y = [], []\n",
        "for doc in docs:\n",
        "    y.append(doc[:1])\n",
        "    X.append(doc[2:])\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "matrix_X = vec.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "kf = KFold(n_splits = 4)\n",
        "nb = MultinomialNB()\n",
        "dt = DecisionTreeClassifier(max_depth = 3)\n",
        "\n",
        "import numpy as np\n",
        "y = np.array(y)\n",
        "\n",
        "f1_nb_score = 0; f1_dt_score = 0\n",
        "for train_ids, test_ids in kf.split(matrix_X):\n",
        "    train_X, test_X = matrix_X[train_ids], matrix_X[test_ids]\n",
        "    train_y, test_y = y[train_ids], y[test_ids]\n",
        "    nb.fit(train_X, train_y)\n",
        "    dt.fit(train_X, train_y)\n",
        "    pred_nb_y = nb.predict(test_X)\n",
        "    pred_dt_y = dt.predict(test_X)\n",
        "    f1_nb_score += f1_score(test_y, pred_nb_y, average = 'micro')\n",
        "    f1_dt_score += f1_score(test_y, pred_dt_y, average = 'micro')\n",
        "    \n",
        "print(f1_nb_score/4)\n",
        "print(f1_dt_score/4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}